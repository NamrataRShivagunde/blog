---
layout: post
slug: busters
author: arogers
title:  "BERT Busters: Outlier Dimensions that Disrupt Transformers"
date:   2021-09-01 21:07:47
tags: transformers 
mathjax: false
toc: true
excerpt: "Models of BERT family are overall robust to pruning, but they have an Achilles heel: the outlier dimensions, without which the quality of the model drops significantly."
og_image: /assets/images/busters-card.png
---

# BERT Busters: outlier dimensions that disrupt Transformers

## Intro: Meet the BERT Busters!

Multiple studies have shown that Transformers are remarkably robust to pruning. You can often afford to lose half the model parameters, both with removing architecture blocks such as heads and layers {% cite PrasannaRogersEtAl_2020_When_BERT_Plays_Lottery_All_Tickets_Are_Winning %} and magnitude-based pruning {% cite ChenFrankleEtAl_2020_Lottery_Ticket_Hypothesis_for_Pre-trained_BERT_Networks %}. Overparametrization makes these models easy to compress afterwards (see surveys on compressing Transformer-based LMs {% cite RogersKovalevaEtAl_2020_Primer_in_BERTology_What_We_Know_About_How_BERT_Works, GaneshChenEtAl_2020_Compressing_Large-Scale_Transformer-Based_Models_Case_Study_on_BERT %}.

Yet pre-trained Transformer encoders do have an Achilles' heel. In our new paper  {% cite kovaleva2021transformer %}, we found that they are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). This effect is present in several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA, and we also found a similar phenomenon in GPT-2. 

We find that across different architectures, the last operation of the Transformer layer has high-magnitude weights in **the same position** in different layers of the model. This results in high-magnitude values in that position in the embeddings computed by the model for 95% of the inputs. We call them outlier features.

In case of BERT and other pre-trained encoder Transformers, the affected components are the scaling factors and biases in the output LayerNorm of unusually high magnitude. Here's what the culprits look like in BERT-base:

<figure>
	<img src="{{'/assets/images/bert-outliers.png' | relative_url }}"> 	
	<figcaption>Fig. 1. Outlier LayerNorm features 308, 381 in BERT-base-uncased (randomly sampled input). </figcaption>
</figure>



## What do the outlier dimensions do?

If we selectively disable these outlier dimensions, the model is severely disrupted. To see what it does to the masked language model, let's ask RoBERTa to fill in some blanks for us. The text in the table below comes from Wikipedia, which the model is well familiar with, and so in line 2, RoBERTa doesn't have any issues filling in the blanks with words that match the original words (light green), or differ from them, but are valid substitutions (brown). In line 3, we disable the same number of random features throughout the model as the number of outliers, but the output is not affected at all. However, in line 4 we see that disabling the outlier features completely disrupts the model, which now mostly produces nonsensical candidates for the masked tokens.

<table>
<tbody>
<tr>
<td>Input&nbsp;</td>
  <td>Ghostbusters was [<span style="color:blue;">released</span>] on June 8 , [<span style="color:blue;">1984</span>] , to critical [<span style="color:blue;">acclaim</span>] and became a cultural phenomenon . It was well [<span style="color:blue;">received</span>] for its deft blend of comedy, [<span style="color:blue;">action</span>] , and horror , and Murray ' s performance was [<span style="color:blue;">repeatedly</span>] singled out for praise .</td>
</tr>
<tr>
<td>RoBERTa&nbsp;</td>
<td>&nbsp;Ghostbusters was [<span style="color:green">released</span>] on June 8 , [<span style="color:#DA8128;">1986</span>] , to critical [<span style="color:green;">acclaim</span>] and became a cultural phenomenon . It was well [<span style="color:green;">received</span>] for its deft blend of comedy,&nbsp; [<span style="color:green;">action</span>] , and horror , and Murray ' s performance was [<span style="color:#DA8128;">often</span>] singled out for praise .</td>
</tr>
<tr>
<td>&nbsp;Random</td>
<td>&nbsp;Ghostbusters was [<span style="color:green">released</span>] on June 8 , [<span style="color:#DA8128;">1986</span>] , to critical [<span style="color:green;">acclaim</span>] and became a cultural phenomenon . It was well [<span style="color:green;">received</span>] for its deft blend of comedy,&nbsp;  [<span style="color:green;">action</span>] , and horror , and Murray ' s performance was [<span style="color:#DA8128;">particularly</span>] singled out for praise.&nbsp;</td>
</tr>
<tr>
<td>&nbsp;Outliers</td>
  <td>&nbsp;<span style="color:red;">{ lock</span> was [<span style="color:red;">never</span>] on June 8 , [<span style="color:red;">&lt;/s&gt;</span>] , to <span style="color:red;">rely</span> [<span style="color:red;">,</span>] and . It was well [<span style="color:#DA8128;">known</span>] for its <span style="color:red;">acker</span> of comedy , [<span style="color:red;">dinner</span>], and horror , and Murray ' s was [<span style="color:red;">ever</span>] <span style="color:red;">, &lt;/s&gt; &lt;/s&gt; ) </span>&nbsp;</td>
</tr>
</tbody>
</table>

The downstream tasks take a hit too. Here is what happens with BERT-base on GLUE when one outlier dimension is disabled at a time:

<figure>
	<img src="{{'/assets/images/outliers-glue.png' | relative_url }}"> 	
	<figcaption>Fig. 2. Performance of BERT-base on GLUE benchmark tasks with output LayerNorm dimensions disabled one at a time. X-axis: which dimensions are disabled. Y-axis: performance metric: loss (blue), accuracy (green), correlation coefficients (purple, orange). </figcaption>
</figure>



## Is this a bug or a feature?

To begin this, the outlier dimensions do not seem to be an artifact of a particular model instance. We found such dimensions in all six models of BERT family that we considered: BERT-base, BERT-small, BERT-medium, BERT-base, mBERT and RoBERTa. They are also found in ELECTRA, XLNet, and BART. A similar phenomenon is present in the output dense layer of GPT-2, (since there the output component is not LayerNorm). It seems that this is a normal effect of pre-training in these models.

To find out when the outliers appear, we pre-train our own  BERT-medium model on BookCorpus {% cite ZhuKirosEtAl_2015_Aligning_Books_and_Movies_Towards_Story-Like_Visual_Explanations_by_Watching_Movies_and_Reading_Books %}. We started from a randomly initialized configuration with 8 layers and the hidden dimensionality of 512 units. We saved checkpoints of the model every 2000 steps, and we tracked the output LayerNorm weights across all of the model‚Äôs layers as the training progressed. Figure 3 shows that both scaling factors and biases begin to diverge from their initialization values quite early (after approximately 50k steps) in the training process. At roughly the same point, both training loss and evaluation perplexity begin to improve, which is in line with the drastic effect on model performance that we saw in the above pruning experiments.

<figure>
	<img src="{{'/assets/images/outliers-pretraining.png' | relative_url }}"> 	
	<figcaption>Fig. 3. BERT-medium pre-training on the BookCorpus dataset. (left) Evaluation perplexity (brown) and train loss (blue) as the training progresses. (middle) The changes in the scaling factors and the biases of the output normalization layer. Each line corresponds to one of the 512 dimensions. We highlight (in orange) the 417-th dimension, for which both the scaling factor and the bias fall out of the three sigma range at the end of pretraining. (right) Token embeddings computed for an input sequence that was randomly sampled from the data. Each line corresponds to one input token. The outlier embedding values are marked at the same 417-th dimension. All the plots are presented for the middle Transformer layer (4). </figcaption>
</figure>



## How is this related to positional embeddings?

In a concurrent work, {% cite luo-etal-2021-positional %} hypothesized that the outlier phenomenon is attributable to the positional embedding layer. We argue {% cite kovaleva2021transformer %} it is much more likely that this effect comes from the LayerNorm component of the embedding layer.

The histogram in Fig 4. shows that it's only the LayerNorm of the embedding layer that has unusual outlier values in the specified dimension. The distribution of weights in the other three components of the embedding layer -- lexical, positional, and segment embeddings -- is centered around zero and forms Gaussian-shaped curves with a small standard deviation.
Compared to the other components, LayerNorm weights have a much higher variance of values, with the highest weight matching the outlier position <tt>308</tt>. We hypothesize that the learned weights of LayerNorm in the embedding layer are responsible for producing high-magnitude outlier features that are propagated through the rest of the network resulting in the consistent outlier effects across the Transformer layers.
	 	
<figure>
	<img src="{{'/assets/images/embedding_weights_marked.png' | relative_url }}"> 	
<figcaption>Fig. 4. The normalized histograms of weight values extracted from different components of the embedding layer of BERT base. The blue histograms represent the entire set of extracted values within a given component, whereas the red ones refer to the weights that result in the outlier feature <tt>308</tt>. Most of the outlier-corresponding weights are centered around zero, while the LayerNorm weights fall out of the 3-ùúé interval.
</figcaption>
</figure>

## Where do we go from here?

So‚Ä¶ if BERT can be completely disrupted by disabling so few weights, how come this phenomenon hasn't been seen in all the pruning studies so far? The catch is that it is not just about the weight magnitude. The outlier dimensions have to be pruned in the exact same position across the model, and neither magnitude pruning nor pruning attention heads based on their importance scores have that constraint. So, simply by not pruning the same positions across the model, the pruning studies have been avoiding the outlier iceberg. Just to avoid degenerate runs by chance, we would recommend that work on pruning/compressing Transformers explicitly puts in the constraint that the pruned weights should not be in the same position across the model.

Now that we know about the outliers, they are an obvious security risk: if a malicious actor gains access to the weights of a model, they could easily identify such weights and modify them (e.g. in a federated learning context). It would look like the model is still running as expected, except that its output would turn into garbage. 

And, since the outlier dimensions seem to be a regular feature of Transformer-based models, this brings up a host of interesting questions for future work:

- Is it possible to pre-train a Transformer that wouldn't have such outliers?
- If they are necessary, shall we save ourselves trouble and initialize the model that way?
- Can their emergence in pre-training be used as an auxiliary signal for when the model training is completed?
- Is 2-3 outlier features necessary sufficient, or will the model quality be improved by creating more of them?





